{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Warehouse Routing Optimization using Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inspiration for a working proof of concept comes from the Research Article titled \"Multi-robot path planning based on a deep reinforcement learning DQN algorithm\" by Yang Yang, et. al. and may be found here: https://ietresearch.onlinelibrary.wiley.com/doi/pdf/10.1049/trit.2020.0024. \n",
    "\n",
    "This proof of concept follows this research article closely, in that, it will show a solution to its main point- optimizing warehouse dispatching systems in order to increase efficiency of the warehouse picking operation. In layman's terms, an optimized route to pick items off of a warehouse shelf will produce a higher throughput rate of goods than a non-optimized route, where the confounding variable of human error is eliminated vis-a-vis a fully robotic warehouse staff. \n",
    "\n",
    "This proof of concept will use a deep Q-network (DQN) algorithm in a deep reinforcement learning algorithm, which combines the Q-learning algorithm, an empirical playback mechanism, and the volume-based technology of productive neural networks to generate target Q-values to solve the problem of path planning. The aim of the Q-learning algorithm in deep reinforcement learning is to address two shortcomings of the path-planning problem: slow convergence and excessive randomness. The simulation results of this PoC will prove that DQN converges faster than the classic deep reinforcement learning algorithm and learns the solutions to path-planning problems faster, thus increasing the efficiency of multi-robot path planning. \n",
    "\n",
    "This PoC comes from my interest in deep reinforcement learning, in that, autonomous agents may make decisions faster with better efficiency and efficacy if used properly. What this PoC will not do is explain the concepts behind deep reinforcement learning. If the reader wishes to learn more about deep reinforcement learning, please use the following Wikipedia link to gain a basic understanding: https://en.wikipedia.org/wiki/Deep_reinforcement_learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Setup\n",
    "\n",
    "There are several concepts that need to be addressed, which will establish a setting in which this problem sits. \n",
    "A warehouse (the environment) consists of at least one worker (the agent) to pick up and transport goods from various points in the warehouse. The agent must decide at every time step (t) which node has been visited and change that node from state=unvisited to state=visited (state). The agent attempts to learn the best order of nodes to visit so the negative total distance (reward) is maximized.\n",
    "\n",
    "MCP concepts here are:\n",
    "- State: the representation of the environment which contains the states (s) such that visited and unvisited are sets of the state.\n",
    "- Action: a finite set of actions (A) in selecting the next unvisited node, such that A may contain the set of all nodes and their states or dependent upon s, such that the set of all possible actions decreases over time. If A=0 at time t, then A must be return to point I/O.\n",
    "- Reward: each reward for all state s to state s' with actions a is always distance between each node and its state except A=0. If A=0, then agent receives a larger reward for completing work.\n",
    "- State-action-transition probability: the probability from state s to state s' under action a, p(s'|s,a)=1 for all preknown state s' and 0 for all other states s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single Agent Proof of Concept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import random\n",
    "\n",
    "def random_states_and_locations(n):\n",
    "    random_states_and_locations = {}\n",
    "    for i in range(n):\n",
    "        random_states_and_locations['key' + str(i)] = random.randrange(n)\n",
    "    random_states_and_locations = {state: location for location, state in random_states_and_locations.items()}\n",
    "    return random_states_and_locations\n",
    "    \n",
    "def random_actions(state):\n",
    "    actions = []\n",
    "    for i in range(len(state)):\n",
    "        actions[i] = random.randrange(10)\n",
    "    return actions\n",
    "\n",
    "def random_rewards():\n",
    "    rewards = np.random.randint(2, size = (12, 12))\n",
    "    return rewards\n",
    "\n",
    "\n",
    "def single_agent_poc_config():\n",
    "    gamma = 0.75\n",
    "    alpha = 0.95\n",
    "    return gamma, alpha\n",
    "    \n",
    "def route(starting_location, ending_location):\n",
    "    new_reward = np.copy(random_rewards)\n",
    "    ending_state = random_states_and_locations[ending_location]\n",
    "    new_reward[ending_state, ending_state] = 1000\n",
    "    q = np.array(np.zeros([12, 12]))\n",
    "    for i in range(1000):\n",
    "        current_state = np.random.randint(10, 12)\n",
    "        playable_actions = []\n",
    "        for j in range(12):\n",
    "            if new_reward[current_state, j] > 0: \n",
    "                playable_actions.append(j)\n",
    "        next_state = np.random.choice(playable_actions)\n",
    "        td = new_reward[current_state, next_state] + single_agent_poc_config.alpha * q[next_state, np.argmax(q[next_state,])] - q[current_state, next_state]\n",
    "        q[current_state, next_state] = q[current_state, next_state] + single_agent_poc_config.alpha * td\n",
    "    route = [starting_location]\n",
    "    next_locaction = starting_location\n",
    "    while (next_location != ending_location):\n",
    "        starting_state = random_states_and_locations[starting_location]\n",
    "        next_state = np.argmax(q[starting_state,])\n",
    "        next_location = random_states_and_locations[next_state]\n",
    "        route.append(next_locaction)\n",
    "        starting_location = next_locaction\n",
    "    return route\n",
    "\n",
    "def best_route(starting_location, intermediary_location, ending_location):\n",
    "    return route(starting_location, intermediary_location) + route(intermediary_location, ending_location)[1:]\n",
    "    print('Route: ')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-agent Path Planning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-agent Path Planning with Collision Avoidance\n",
    "\n",
    "https://github.com/LyapunovJingci/Warehouse_Robot_Path_Planning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "39856827656c20989eeeffcf95a407f9e13b5606a13e09624e8ebfa8af1452ea"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
